{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction  \n",
    "\n",
    "Now that you've seen some basic linear regression models it's time to discuss further how to better tune these models. As you saw, we usually begin with an error or loss function for which we'll apply an optimization algorithm such as gradient descent. We then apply this optimization algorithm to the error function we're trying to minimize and voila, we have an optimized solution! Unfortunately, things aren't quite that simple. \n",
    "\n",
    "### Overfitting and Underfitting\n",
    "Most importantly is the issue of generalization.\n",
    "This is often examined by discussing underfitting and overfitting.\n",
    "![](./images/overfit_underfit.png)\n",
    "\n",
    "Recall our main goal when performing regression: we're attempting to find relationships that can generalize to new cases. Generally, the more data that we have the better off we'll be as we can observe more patterns and relationships within that data. However, some of these patterns and relationships may not generalize well to other cases. \n",
    "\n",
    "\n",
    "### Systems of Equations\n",
    "If you recall from your earlier life as a algebra student:\n",
    "\n",
    "$2x +10 = 18$ has a unique solution; one variable, one equation, one solution\n",
    "\n",
    "Similarly, two variables with two equations has one solution*   \n",
    "$x+y=4$\n",
    "$2x+2y=10$\n",
    "\n",
    "However, if we allow 2 variables with only 1 equation, we can have infinite solutions.\n",
    "$x+y=4$\n",
    "\n",
    "*(An inconsistent system will have no solution and a system where the second equation is a multiple of the first will have infinite solutions)\n",
    "\n",
    "### Fundamental Theorem of Algebra\n",
    "http://mathworld.wolfram.com/FundamentalTheoremofAlgebra.html\n",
    "\n",
    "This is setting us for a wonderful restatement of the fundamental theorem of algebra!\n",
    "If we allow ourselves more variables then we have equations (more variables then observances), then we can have infinite solutions.\n",
    "\n",
    "If I have 30 observed data points, I can make a model that will perfectly go through all 30 points using a degree 30 polynomial. But such a complex model may not generalize well.\n",
    "\n",
    "\n",
    "### Normalization\n",
    "\n",
    "So with all of this, what do we do? What we've seen is that an error function alone may not prove sufficient. Yes, we want to have good performance, but we also want our model to hold up for future cases that may not be within the data we have available. A common method in normalizing models therefore is to penalize the number of terms allowed within the model. This also captures the intuition behind Occam's razor, that we should choose a simpler explanation when presented with two options that produce comparable results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L^p$ norm of x\n",
    "In order to help account for underfitting and overfitting, we often use what are called $L^p$ norms.   \n",
    "The **$L^p$ norm of x** is defined as:  \n",
    "\n",
    "### $||x||_p  =  \\big(\\sum_{i} x_i^p\\big)^\\frac{1}{p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the L1 and L2 Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge (L2)\n",
    "One common normalization is called Ridge Regression and uses the $l_2$ norm (also known as the Euclidean norm) as defined above.   \n",
    "The ridge coefficients minimize a penalized residual sum of squares:    \n",
    "    $ \\sum(\\hat{y}-y)^2 + \\lambda\\bullet w^2$\n",
    "\n",
    "Write this loss function for performing ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your function goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso (L1)\n",
    "Another common normalization is called Lasso Regression and uses the $l_1$ norm.   \n",
    "The ridge coefficients minimize a penalized residual sum of squares:    \n",
    "    $ \\sum(\\hat{y}-y)^2 + \\lambda\\bullet |w|$\n",
    "\n",
    "Write this loss function for performing ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Elastic Neat - Mean of L1 + L2 Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance Tradeoffs\n",
    "\n",
    "### Choosing $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
