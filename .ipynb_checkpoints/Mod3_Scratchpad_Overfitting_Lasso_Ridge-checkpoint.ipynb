{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Functions\n",
    "\n",
    "Expanding this example, we can imagine being in charge of managing a large production and we might want to figure out what other factors such as the actors, director, genre, running length or other features were most predictive of the movie's success in the box office.\n",
    "\n",
    "Here gradient descent is slightly more complicated as we're dealing with the multivariate case. As a result, we can take the derivative (or gradient) with respect to different variables. The underlying intuition is that we want to move in the direction of the steepest descent in hopes that we can find the global minimum. We then do this through a series of successive steepest steps forward until we are satisfied with the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction  \n",
    "\n",
    "Now that you've seen some basic linear regression models it's time to discuss further how to better tune these models. As you saw, we usually begin with an error or loss function for which we'll apply an optimization algorithm such as gradient descent. We then apply this optimization algorithm to the error function we're trying to minimize and voila, we have an optimized solution! Unfortunately, things aren't quite that simple. \n",
    "\n",
    "### Overfitting and Underfitting\n",
    "Most importantly is the issue of generalization.\n",
    "This is often examined by discussing underfitting and overfitting.\n",
    "![](./images/overfit_underfit.png)\n",
    "\n",
    "Recall our main goal when performing regression: we're attempting to find relationships that can generalize to new cases. Generally, the more data that we have the better off we'll be as we can observe more patterns and relationships within that data. However, some of these patterns and relationships may not generalize well to other cases. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Fundamental Theorem of Algebra\n",
    "http://mathworld.wolfram.com/FundamentalTheoremofAlgebra.html\n",
    "\n",
    "This is setting us for a wonderful restatement of the fundamental theorem of algebra!\n",
    "If we allow ourselves more variables then we have equations (more variables then observances), then we can have infinite solutions.\n",
    "\n",
    "If I have 30 observed data points, I can make a model that will perfectly go through all 30 points using a degree 30 polynomial. But such a complex model may not generalize well.\n",
    "\n",
    "\n",
    "### Normalization\n",
    "\n",
    "So with all of this, what do we do? What we've seen is that an error function alone may not prove sufficient. Yes, we want to have good performance, but we also want our model to hold up for future cases that may not be within the data we have available. A common method in normalizing models therefore is to penalize the number of terms allowed within the model. This also captures the intuition behind Occam's razor, that we should choose a simpler explanation when presented with two options that produce comparable results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L^p$ norm of x\n",
    "In order to help account for underfitting and overfitting, we often use what are called $L^p$ norms.   \n",
    "The **$L^p$ norm of x** is defined as:  \n",
    "\n",
    "### $||x||_p  =  \\big(\\sum_{i} x_i^p\\big)^\\frac{1}{p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the L1 and L2 Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge (L2)\n",
    "One common normalization is called Ridge Regression and uses the $l_2$ norm (also known as the Euclidean norm) as defined above.   \n",
    "The ridge coefficients minimize a penalized residual sum of squares:    \n",
    "    $ \\sum(\\hat{y}-y)^2 + \\lambda\\bullet w^2$\n",
    "\n",
    "Write this loss function for performing ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your function goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso (L1)\n",
    "Another common normalization is called Lasso Regression and uses the $l_1$ norm.   \n",
    "The ridge coefficients minimize a penalized residual sum of squares:    \n",
    "    $ \\sum(\\hat{y}-y)^2 + \\lambda\\bullet |w|$\n",
    "\n",
    "Write this loss function for performing ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Elastic Net - Mean of L1 + L2 Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance Tradeoffs\n",
    "\n",
    "### Choosing $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "http://scikit-learn.org/stable/modules/linear_model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
